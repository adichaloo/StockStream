2023-12-01 16:30:19,065:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
2023-12-01 16:30:19,066:check_version:INFO:Probing node bootstrap-0 broker version
2023-12-01 16:30:19,066:connect:ERROR:Connect attempt to <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]> returned error 61. Disconnecting.
2023-12-01 16:30:19,066:close:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. KafkaConnectionError: 61 ECONNREFUSED
2023-12-01 16:30:19,113:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
2023-12-01 16:30:19,113:connect:ERROR:Connect attempt to <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]> returned error 61. Disconnecting.
2023-12-01 16:30:19,113:close:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. KafkaConnectionError: 61 ECONNREFUSED
2023-12-01 16:30:19,163:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: connecting to localhost:9092 [('127.0.0.1', 9092) IPv4]
2023-12-01 16:30:19,163:connect:ERROR:Connect attempt to <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]> returned error 61. Disconnecting.
2023-12-01 16:30:19,163:close:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: Closing connection. KafkaConnectionError: 61 ECONNREFUSED
2023-12-01 16:30:19,163:<module>:ERROR:NoBrokersAvailable
2023-12-01 16:30:19,163:StockPricePrediction:ERROR:name 'Consumer' is not defined
2023-12-01 16:30:19,172:_log:INFO:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:8080
 * Running on http://172.31.254.167:8080
2023-12-01 16:30:19,172:_log:INFO:[33mPress CTRL+C to quit[0m
2023-12-01 16:30:19,172:_log:INFO: * Restarting with stat
2023-12-01 16:30:23,733:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
2023-12-01 16:30:23,733:check_version:INFO:Probing node bootstrap-0 broker version
2023-12-01 16:30:23,733:connect:ERROR:Connect attempt to <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]> returned error 61. Disconnecting.
2023-12-01 16:30:23,733:close:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. KafkaConnectionError: 61 ECONNREFUSED
2023-12-01 16:30:23,780:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
2023-12-01 16:30:23,781:connect:ERROR:Connect attempt to <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]> returned error 61. Disconnecting.
2023-12-01 16:30:23,781:close:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. KafkaConnectionError: 61 ECONNREFUSED
2023-12-01 16:30:23,830:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: connecting to localhost:9092 [('127.0.0.1', 9092) IPv4]
2023-12-01 16:30:23,831:connect:ERROR:Connect attempt to <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]> returned error 61. Disconnecting.
2023-12-01 16:30:23,831:close:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: Closing connection. KafkaConnectionError: 61 ECONNREFUSED
2023-12-01 16:30:23,831:<module>:ERROR:NoBrokersAvailable
2023-12-01 16:30:23,831:StockPricePrediction:ERROR:name 'Consumer' is not defined
2023-12-01 16:30:23,837:_log:WARNING: * Debugger is active!
2023-12-01 16:30:23,851:_log:INFO: * Debugger PIN: 514-370-077
2023-12-01 16:42:15,826:dataGrabber:INFO:The AlphaVantage API key must be provided either through the key parameter or through the environment variable ALPHAVANTAGE_API_KEY. Get a free key from the alphavantage website: https://www.alphavantage.co/support/#api-key
2023-12-01 16:42:56,292:dataGrabber:INFO:The AlphaVantage API key must be provided either through the key parameter or through the environment variable ALPHAVANTAGE_API_KEY. Get a free key from the alphavantage website: https://www.alphavantage.co/support/#api-key
2023-12-01 16:43:06,762:dataGrabber:INFO:The AlphaVantage API key must be provided either through the key parameter or through the environment variable ALPHAVANTAGE_API_KEY. Get a free key from the alphavantage website: https://www.alphavantage.co/support/#api-key
2023-12-01 16:43:31,956:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
2023-12-01 16:43:31,957:check_version:INFO:Probing node bootstrap-0 broker version
2023-12-01 16:43:31,957:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
2023-12-01 16:43:32,085:check_version:INFO:Broker version identified as 2.5.0
2023-12-01 16:43:32,085:check_version:INFO:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup
2023-12-01 16:43:32,086:__init__:WARNING:group_id is None: disabling auto-commit.
2023-12-01 16:43:32,086:change_subscription:INFO:Updating subscribed topics to: ('stock_data',)
2023-12-01 16:43:32,223:assign_from_subscribed:INFO:Updated partition assignment: [TopicPartition(topic='stock_data', partition=0)]
2023-12-01 16:43:32,228:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: connecting to vl965-172-31-254-167.wireless.umass.edu:9092 [('172.31.254.167', 9092) IPv4]
2023-12-01 16:43:32,332:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: Connection complete.
2023-12-01 16:43:32,332:close:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
2023-12-01 16:49:22,682:close:INFO:Closing down clientserver connection
2023-12-01 16:49:27,856:dataGrabber:INFO:The AlphaVantage API key must be provided either through the key parameter or through the environment variable ALPHAVANTAGE_API_KEY. Get a free key from the alphavantage website: https://www.alphavantage.co/support/#api-key
2023-12-01 16:51:19,531:dataGrabber:INFO:The AlphaVantage API key must be provided either through the key parameter or through the environment variable ALPHAVANTAGE_API_KEY. Get a free key from the alphavantage website: https://www.alphavantage.co/support/#api-key
2023-12-01 16:53:43,344:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
2023-12-01 16:53:43,344:check_version:INFO:Probing node bootstrap-0 broker version
2023-12-01 16:53:43,345:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
2023-12-01 16:53:43,453:check_version:INFO:Broker version identified as 2.5.0
2023-12-01 16:53:43,453:check_version:INFO:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup
2023-12-01 16:53:43,458:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: connecting to vl965-172-31-254-167.wireless.umass.edu:9092 [('172.31.254.167', 9092) IPv4]
2023-12-01 16:53:43,458:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: Connection complete.
2023-12-01 16:53:43,458:close:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
2023-12-01 16:54:27,105:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
2023-12-01 16:54:27,106:check_version:INFO:Probing node bootstrap-0 broker version
2023-12-01 16:54:27,106:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
2023-12-01 16:54:27,215:check_version:INFO:Broker version identified as 2.5.0
2023-12-01 16:54:27,215:check_version:INFO:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup
2023-12-01 16:54:27,215:__init__:WARNING:group_id is None: disabling auto-commit.
2023-12-01 16:54:27,215:change_subscription:INFO:Updating subscribed topics to: ('stock_data',)
2023-12-01 16:54:27,217:assign_from_subscribed:INFO:Updated partition assignment: [TopicPartition(topic='stock_data', partition=0)]
2023-12-01 16:54:27,220:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: connecting to vl965-172-31-254-167.wireless.umass.edu:9092 [('172.31.254.167', 9092) IPv4]
2023-12-01 16:54:27,325:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: Connection complete.
2023-12-01 16:54:27,325:close:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
2023-12-01 16:54:39,577:StockPricePrediction:ERROR:An error occurred while calling o145.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3) (vl965-172-31-254-167.wireless.umass.edu executor driver): org.apache.spark.SparkException: 
Bad data in pyspark.daemon's standard output. Invalid port number:
  1231975525 (0x496e7465)
Python command to execute the daemon was:
  python3 -m pyspark.daemon
Check that you don't have any unexpected modules or libraries in
your PYTHONPATH:
  /Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip:/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip:/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/jars/spark-core_2.12-3.5.0.jar
Also, check if you have a sitecustomize.py module in your python path,
or in your python installation, that is printing to standard output
	at org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:267)
	at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)
	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3549)
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:315)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: org.apache.spark.SparkException: 
Bad data in pyspark.daemon's standard output. Invalid port number:
  1231975525 (0x496e7465)
Python command to execute the daemon was:
  python3 -m pyspark.daemon
Check that you don't have any unexpected modules or libraries in
your PYTHONPATH:
  /Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip:/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip:/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/jars/spark-core_2.12-3.5.0.jar
Also, check if you have a sitecustomize.py module in your python path,
or in your python installation, that is printing to standard output
	at org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:267)
	at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)
	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	... 1 more

2023-12-01 16:54:39,589:_log:INFO:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:8080
 * Running on http://172.31.254.167:8080
2023-12-01 16:54:39,589:_log:INFO:[33mPress CTRL+C to quit[0m
2023-12-01 16:54:39,590:_log:INFO: * Restarting with stat
2023-12-01 16:54:44,197:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
2023-12-01 16:54:44,197:check_version:INFO:Probing node bootstrap-0 broker version
2023-12-01 16:54:44,198:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
2023-12-01 16:54:44,301:check_version:INFO:Broker version identified as 2.5.0
2023-12-01 16:54:44,301:check_version:INFO:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup
2023-12-01 16:54:44,301:__init__:WARNING:group_id is None: disabling auto-commit.
2023-12-01 16:54:44,301:change_subscription:INFO:Updating subscribed topics to: ('stock_data',)
2023-12-01 16:54:44,303:assign_from_subscribed:INFO:Updated partition assignment: [TopicPartition(topic='stock_data', partition=0)]
2023-12-01 16:54:44,306:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: connecting to vl965-172-31-254-167.wireless.umass.edu:9092 [('172.31.254.167', 9092) IPv4]
2023-12-01 16:54:44,412:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: Connection complete.
2023-12-01 16:54:44,413:close:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
2023-12-01 16:54:57,546:StockPricePrediction:ERROR:An error occurred while calling o145.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3) (vl965-172-31-254-167.wireless.umass.edu executor driver): org.apache.spark.SparkException: 
Bad data in pyspark.daemon's standard output. Invalid port number:
  1231975525 (0x496e7465)
Python command to execute the daemon was:
  python3 -m pyspark.daemon
Check that you don't have any unexpected modules or libraries in
your PYTHONPATH:
  /Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip:/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip:/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/jars/spark-core_2.12-3.5.0.jar
Also, check if you have a sitecustomize.py module in your python path,
or in your python installation, that is printing to standard output
	at org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:267)
	at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)
	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3549)
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:315)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: org.apache.spark.SparkException: 
Bad data in pyspark.daemon's standard output. Invalid port number:
  1231975525 (0x496e7465)
Python command to execute the daemon was:
  python3 -m pyspark.daemon
Check that you don't have any unexpected modules or libraries in
your PYTHONPATH:
  /Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip:/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip:/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/jars/spark-core_2.12-3.5.0.jar
Also, check if you have a sitecustomize.py module in your python path,
or in your python installation, that is printing to standard output
	at org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:267)
	at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)
	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	... 1 more

2023-12-01 16:54:57,556:_log:WARNING: * Debugger is active!
2023-12-01 16:54:57,576:_log:INFO: * Debugger PIN: 514-370-077
2023-12-01 16:56:22,040:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
2023-12-01 16:56:22,040:check_version:INFO:Probing node bootstrap-0 broker version
2023-12-01 16:56:22,041:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
2023-12-01 16:56:22,150:check_version:INFO:Broker version identified as 2.5.0
2023-12-01 16:56:22,150:check_version:INFO:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup
2023-12-01 16:56:22,153:__init__:WARNING:group_id is None: disabling auto-commit.
2023-12-01 16:56:22,153:change_subscription:INFO:Updating subscribed topics to: ('stock_data',)
2023-12-01 16:56:22,155:assign_from_subscribed:INFO:Updated partition assignment: [TopicPartition(topic='stock_data', partition=0)]
2023-12-01 16:56:22,158:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: connecting to vl965-172-31-254-167.wireless.umass.edu:9092 [('172.31.254.167', 9092) IPv4]
2023-12-01 16:56:22,264:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: Connection complete.
2023-12-01 16:56:22,264:close:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
2023-12-01 16:56:33,644:StockPricePrediction:ERROR:An error occurred while calling o145.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3) (vl965-172-31-254-167.wireless.umass.edu executor driver): org.apache.spark.SparkException: 
Bad data in pyspark.daemon's standard output. Invalid port number:
  1231975525 (0x496e7465)
Python command to execute the daemon was:
  python3 -m pyspark.daemon
Check that you don't have any unexpected modules or libraries in
your PYTHONPATH:
  /Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip:/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip:/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/jars/spark-core_2.12-3.5.0.jar
Also, check if you have a sitecustomize.py module in your python path,
or in your python installation, that is printing to standard output
	at org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:267)
	at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)
	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3549)
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:315)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: org.apache.spark.SparkException: 
Bad data in pyspark.daemon's standard output. Invalid port number:
  1231975525 (0x496e7465)
Python command to execute the daemon was:
  python3 -m pyspark.daemon
Check that you don't have any unexpected modules or libraries in
your PYTHONPATH:
  /Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip:/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip:/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/jars/spark-core_2.12-3.5.0.jar
Also, check if you have a sitecustomize.py module in your python path,
or in your python installation, that is printing to standard output
	at org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:267)
	at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)
	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	... 1 more

2023-12-01 16:56:33,646:close:INFO:Closing down clientserver connection
2023-12-01 16:58:13,289:_log:INFO: * Detected change in '/Users/mnswdhw/Documents/Courses/CS532/Stock_Data_Prediction/Stock_Data_Prediction_Of_Live_Data/Consumer.py', reloading
2023-12-01 16:58:13,292:close:INFO:Closing down clientserver connection
2023-12-01 16:58:13,397:_log:INFO: * Restarting with stat
2023-12-01 16:58:18,184:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
2023-12-01 16:58:18,184:check_version:INFO:Probing node bootstrap-0 broker version
2023-12-01 16:58:18,185:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
2023-12-01 16:58:18,290:check_version:INFO:Broker version identified as 2.5.0
2023-12-01 16:58:18,290:check_version:INFO:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup
2023-12-01 16:58:18,290:__init__:WARNING:group_id is None: disabling auto-commit.
2023-12-01 16:58:18,290:change_subscription:INFO:Updating subscribed topics to: ('stock_data',)
2023-12-01 16:58:18,292:assign_from_subscribed:INFO:Updated partition assignment: [TopicPartition(topic='stock_data', partition=0)]
2023-12-01 16:58:18,294:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: connecting to vl965-172-31-254-167.wireless.umass.edu:9092 [('172.31.254.167', 9092) IPv4]
2023-12-01 16:58:18,398:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: Connection complete.
2023-12-01 16:58:18,398:close:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
2023-12-01 16:58:30,838:StockPricePrediction:ERROR:An error occurred while calling o145.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3) (vl965-172-31-254-167.wireless.umass.edu executor driver): org.apache.spark.SparkException: 
Bad data in pyspark.daemon's standard output. Invalid port number:
  1231975525 (0x496e7465)
Python command to execute the daemon was:
  python3 -m pyspark.daemon
Check that you don't have any unexpected modules or libraries in
your PYTHONPATH:
  /Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip:/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip:/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/jars/spark-core_2.12-3.5.0.jar
Also, check if you have a sitecustomize.py module in your python path,
or in your python installation, that is printing to standard output
	at org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:267)
	at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)
	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3549)
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:315)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: org.apache.spark.SparkException: 
Bad data in pyspark.daemon's standard output. Invalid port number:
  1231975525 (0x496e7465)
Python command to execute the daemon was:
  python3 -m pyspark.daemon
Check that you don't have any unexpected modules or libraries in
your PYTHONPATH:
  /Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip:/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip:/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/jars/spark-core_2.12-3.5.0.jar
Also, check if you have a sitecustomize.py module in your python path,
or in your python installation, that is printing to standard output
	at org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:267)
	at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)
	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	... 1 more

2023-12-01 16:58:30,848:_log:WARNING: * Debugger is active!
2023-12-01 16:58:30,864:_log:INFO: * Debugger PIN: 514-370-077
2023-12-01 16:58:55,088:_log:INFO: * Detected change in '/Users/mnswdhw/Documents/Courses/CS532/Stock_Data_Prediction/Stock_Data_Prediction_Of_Live_Data/Consumer.py', reloading
2023-12-01 16:58:55,090:close:INFO:Closing down clientserver connection
2023-12-01 16:58:55,212:_log:INFO: * Restarting with stat
2023-12-01 16:58:59,920:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
2023-12-01 16:58:59,920:check_version:INFO:Probing node bootstrap-0 broker version
2023-12-01 16:58:59,921:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
2023-12-01 16:59:00,024:check_version:INFO:Broker version identified as 2.5.0
2023-12-01 16:59:00,024:check_version:INFO:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup
2023-12-01 16:59:00,025:__init__:WARNING:group_id is None: disabling auto-commit.
2023-12-01 16:59:00,025:change_subscription:INFO:Updating subscribed topics to: ('stock_data',)
2023-12-01 16:59:00,026:assign_from_subscribed:INFO:Updated partition assignment: [TopicPartition(topic='stock_data', partition=0)]
2023-12-01 16:59:00,029:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: connecting to vl965-172-31-254-167.wireless.umass.edu:9092 [('172.31.254.167', 9092) IPv4]
2023-12-01 16:59:00,134:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: Connection complete.
2023-12-01 16:59:00,134:close:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
2023-12-01 16:59:12,882:StockPricePrediction:ERROR:An error occurred while calling o145.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3) (vl965-172-31-254-167.wireless.umass.edu executor driver): org.apache.spark.SparkException: 
Bad data in pyspark.daemon's standard output. Invalid port number:
  1231975525 (0x496e7465)
Python command to execute the daemon was:
  python3 -m pyspark.daemon
Check that you don't have any unexpected modules or libraries in
your PYTHONPATH:
  /Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip:/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip:/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/jars/spark-core_2.12-3.5.0.jar
Also, check if you have a sitecustomize.py module in your python path,
or in your python installation, that is printing to standard output
	at org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:267)
	at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)
	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3549)
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:315)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: org.apache.spark.SparkException: 
Bad data in pyspark.daemon's standard output. Invalid port number:
  1231975525 (0x496e7465)
Python command to execute the daemon was:
  python3 -m pyspark.daemon
Check that you don't have any unexpected modules or libraries in
your PYTHONPATH:
  /Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip:/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip:/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/jars/spark-core_2.12-3.5.0.jar
Also, check if you have a sitecustomize.py module in your python path,
or in your python installation, that is printing to standard output
	at org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:267)
	at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)
	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	... 1 more

2023-12-01 16:59:12,893:_log:WARNING: * Debugger is active!
2023-12-01 16:59:12,911:_log:INFO: * Debugger PIN: 514-370-077
2023-12-01 17:00:00,248:close:INFO:Closing down clientserver connection
2023-12-01 17:00:00,377:close:INFO:Closing down clientserver connection
2023-12-01 17:00:06,718:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
2023-12-01 17:00:06,719:check_version:INFO:Probing node bootstrap-0 broker version
2023-12-01 17:00:06,719:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
2023-12-01 17:00:06,824:check_version:INFO:Broker version identified as 2.5.0
2023-12-01 17:00:06,825:check_version:INFO:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup
2023-12-01 17:00:06,825:__init__:WARNING:group_id is None: disabling auto-commit.
2023-12-01 17:00:06,825:change_subscription:INFO:Updating subscribed topics to: ('stock_data',)
2023-12-01 17:00:06,827:assign_from_subscribed:INFO:Updated partition assignment: [TopicPartition(topic='stock_data', partition=0)]
2023-12-01 17:00:06,829:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: connecting to vl965-172-31-254-167.wireless.umass.edu:9092 [('172.31.254.167', 9092) IPv4]
2023-12-01 17:00:06,935:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: Connection complete.
2023-12-01 17:00:06,936:close:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
2023-12-01 17:00:18,973:StockPricePrediction:ERROR:An error occurred while calling o145.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3) (vl965-172-31-254-167.wireless.umass.edu executor driver): org.apache.spark.SparkException: 
Bad data in pyspark.daemon's standard output. Invalid port number:
  1231975525 (0x496e7465)
Python command to execute the daemon was:
  python3 -m pyspark.daemon
Check that you don't have any unexpected modules or libraries in
your PYTHONPATH:
  /Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip:/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip:/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/jars/spark-core_2.12-3.5.0.jar
Also, check if you have a sitecustomize.py module in your python path,
or in your python installation, that is printing to standard output
	at org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:267)
	at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)
	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3549)
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:315)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: org.apache.spark.SparkException: 
Bad data in pyspark.daemon's standard output. Invalid port number:
  1231975525 (0x496e7465)
Python command to execute the daemon was:
  python3 -m pyspark.daemon
Check that you don't have any unexpected modules or libraries in
your PYTHONPATH:
  /Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip:/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip:/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/jars/spark-core_2.12-3.5.0.jar
Also, check if you have a sitecustomize.py module in your python path,
or in your python installation, that is printing to standard output
	at org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:267)
	at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)
	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	... 1 more

2023-12-01 17:00:18,983:_log:INFO:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:8080
 * Running on http://172.31.254.167:8080
2023-12-01 17:00:18,983:_log:INFO:[33mPress CTRL+C to quit[0m
2023-12-01 17:00:18,984:_log:INFO: * Restarting with stat
2023-12-01 17:00:23,308:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
2023-12-01 17:00:23,308:check_version:INFO:Probing node bootstrap-0 broker version
2023-12-01 17:00:23,309:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
2023-12-01 17:00:23,416:check_version:INFO:Broker version identified as 2.5.0
2023-12-01 17:00:23,417:check_version:INFO:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup
2023-12-01 17:00:23,417:__init__:WARNING:group_id is None: disabling auto-commit.
2023-12-01 17:00:23,417:change_subscription:INFO:Updating subscribed topics to: ('stock_data',)
2023-12-01 17:00:23,419:assign_from_subscribed:INFO:Updated partition assignment: [TopicPartition(topic='stock_data', partition=0)]
2023-12-01 17:00:23,421:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: connecting to vl965-172-31-254-167.wireless.umass.edu:9092 [('172.31.254.167', 9092) IPv4]
2023-12-01 17:00:23,524:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: Connection complete.
2023-12-01 17:00:23,525:close:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
2023-12-01 17:00:36,994:StockPricePrediction:ERROR:An error occurred while calling o145.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3) (vl965-172-31-254-167.wireless.umass.edu executor driver): org.apache.spark.SparkException: 
Bad data in pyspark.daemon's standard output. Invalid port number:
  1231975525 (0x496e7465)
Python command to execute the daemon was:
  python3 -m pyspark.daemon
Check that you don't have any unexpected modules or libraries in
your PYTHONPATH:
  /Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip:/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip:/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/jars/spark-core_2.12-3.5.0.jar
Also, check if you have a sitecustomize.py module in your python path,
or in your python installation, that is printing to standard output
	at org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:267)
	at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)
	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3549)
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:315)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: org.apache.spark.SparkException: 
Bad data in pyspark.daemon's standard output. Invalid port number:
  1231975525 (0x496e7465)
Python command to execute the daemon was:
  python3 -m pyspark.daemon
Check that you don't have any unexpected modules or libraries in
your PYTHONPATH:
  /Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip:/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip:/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/jars/spark-core_2.12-3.5.0.jar
Also, check if you have a sitecustomize.py module in your python path,
or in your python installation, that is printing to standard output
	at org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:267)
	at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)
	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	... 1 more

2023-12-01 17:00:37,002:_log:WARNING: * Debugger is active!
2023-12-01 17:00:37,018:_log:INFO: * Debugger PIN: 514-370-077
2023-12-01 17:02:04,256:close:INFO:Closing down clientserver connection
2023-12-01 17:02:04,434:close:INFO:Closing down clientserver connection
2023-12-01 17:03:27,425:close:INFO:Closing the Kafka producer with 0 secs timeout.
2023-12-01 17:03:27,425:close:INFO:Proceeding to force close the producer since pending requests could not be completed within timeout 0.
2023-12-01 17:03:58,319:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
2023-12-01 17:03:58,319:check_version:INFO:Probing node bootstrap-0 broker version
2023-12-01 17:03:58,320:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
2023-12-01 17:03:58,427:check_version:INFO:Broker version identified as 2.5.0
2023-12-01 17:03:58,427:check_version:INFO:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup
2023-12-01 17:03:58,428:__init__:WARNING:group_id is None: disabling auto-commit.
2023-12-01 17:03:58,428:change_subscription:INFO:Updating subscribed topics to: ('stock_data',)
2023-12-01 17:03:58,430:assign_from_subscribed:INFO:Updated partition assignment: [TopicPartition(topic='stock_data', partition=0)]
2023-12-01 17:03:58,433:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: connecting to vl965-172-31-254-167.wireless.umass.edu:9092 [('172.31.254.167', 9092) IPv4]
2023-12-01 17:03:58,538:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: Connection complete.
2023-12-01 17:03:58,538:close:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
2023-12-01 17:04:36,048:close:INFO:Closing down clientserver connection
2023-12-01 17:04:55,329:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
2023-12-01 17:04:55,330:check_version:INFO:Probing node bootstrap-0 broker version
2023-12-01 17:04:55,330:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
2023-12-01 17:04:55,433:check_version:INFO:Broker version identified as 2.5.0
2023-12-01 17:04:55,433:check_version:INFO:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup
2023-12-01 17:04:55,434:__init__:WARNING:group_id is None: disabling auto-commit.
2023-12-01 17:04:55,434:change_subscription:INFO:Updating subscribed topics to: ('stock_data',)
2023-12-01 17:04:55,435:assign_from_subscribed:INFO:Updated partition assignment: [TopicPartition(topic='stock_data', partition=0)]
2023-12-01 17:04:55,437:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: connecting to vl965-172-31-254-167.wireless.umass.edu:9092 [('172.31.254.167', 9092) IPv4]
2023-12-01 17:04:55,543:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: Connection complete.
2023-12-01 17:04:55,543:close:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
2023-12-01 17:05:08,574:close:INFO:Closing down clientserver connection
2023-12-01 17:06:39,873:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
2023-12-01 17:06:39,873:check_version:INFO:Probing node bootstrap-0 broker version
2023-12-01 17:06:39,874:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
2023-12-01 17:06:39,984:check_version:INFO:Broker version identified as 2.5.0
2023-12-01 17:06:39,984:check_version:INFO:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup
2023-12-01 17:06:39,984:__init__:WARNING:group_id is None: disabling auto-commit.
2023-12-01 17:06:39,985:change_subscription:INFO:Updating subscribed topics to: ('stock_data',)
2023-12-01 17:06:39,986:assign_from_subscribed:INFO:Updated partition assignment: [TopicPartition(topic='stock_data', partition=0)]
2023-12-01 17:06:39,989:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: connecting to vl965-172-31-254-167.wireless.umass.edu:9092 [('172.31.254.167', 9092) IPv4]
2023-12-01 17:06:40,091:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: Connection complete.
2023-12-01 17:06:40,092:close:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
2023-12-01 17:06:58,628:close:INFO:Closing down clientserver connection
2023-12-01 17:09:45,762:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
2023-12-01 17:09:45,763:check_version:INFO:Probing node bootstrap-0 broker version
2023-12-01 17:09:45,763:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
2023-12-01 17:09:45,893:check_version:INFO:Broker version identified as 2.5.0
2023-12-01 17:09:45,893:check_version:INFO:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup
2023-12-01 17:09:45,899:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: connecting to vl965-172-31-254-167.wireless.umass.edu:9092 [('172.31.254.167', 9092) IPv4]
2023-12-01 17:09:45,899:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: Connection complete.
2023-12-01 17:09:45,899:close:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
2023-12-01 17:10:10,145:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
2023-12-01 17:10:10,145:check_version:INFO:Probing node bootstrap-0 broker version
2023-12-01 17:10:10,146:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
2023-12-01 17:10:10,254:check_version:INFO:Broker version identified as 2.5.0
2023-12-01 17:10:10,254:check_version:INFO:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup
2023-12-01 17:10:10,254:__init__:WARNING:group_id is None: disabling auto-commit.
2023-12-01 17:10:10,254:change_subscription:INFO:Updating subscribed topics to: ('stock_data',)
2023-12-01 17:10:10,258:assign_from_subscribed:INFO:Updated partition assignment: [TopicPartition(topic='stock_data', partition=0)]
2023-12-01 17:10:10,261:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: connecting to vl965-172-31-254-167.wireless.umass.edu:9092 [('172.31.254.167', 9092) IPv4]
2023-12-01 17:10:10,366:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: Connection complete.
2023-12-01 17:10:10,367:close:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
2023-12-01 17:10:23,944:StockPricePrediction:ERROR:An error occurred while calling o145.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3) (vl965-172-31-254-167.wireless.umass.edu executor driver): org.apache.spark.SparkException: 
Bad data in pyspark.daemon's standard output. Invalid port number:
  1231975525 (0x496e7465)
Python command to execute the daemon was:
  python3 -m pyspark.daemon
Check that you don't have any unexpected modules or libraries in
your PYTHONPATH:
  /Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip:/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip:/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/jars/spark-core_2.12-3.5.0.jar
Also, check if you have a sitecustomize.py module in your python path,
or in your python installation, that is printing to standard output
	at org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:267)
	at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)
	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3549)
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:315)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: org.apache.spark.SparkException: 
Bad data in pyspark.daemon's standard output. Invalid port number:
  1231975525 (0x496e7465)
Python command to execute the daemon was:
  python3 -m pyspark.daemon
Check that you don't have any unexpected modules or libraries in
your PYTHONPATH:
  /Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip:/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip:/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/jars/spark-core_2.12-3.5.0.jar
Also, check if you have a sitecustomize.py module in your python path,
or in your python installation, that is printing to standard output
	at org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:267)
	at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)
	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	... 1 more

2023-12-01 17:10:23,955:_log:INFO:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:8080
 * Running on http://172.31.254.167:8080
2023-12-01 17:10:23,955:_log:INFO:[33mPress CTRL+C to quit[0m
2023-12-01 17:10:23,956:_log:INFO: * Restarting with stat
2023-12-01 17:10:28,496:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
2023-12-01 17:10:28,496:check_version:INFO:Probing node bootstrap-0 broker version
2023-12-01 17:10:28,497:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
2023-12-01 17:10:28,605:check_version:INFO:Broker version identified as 2.5.0
2023-12-01 17:10:28,605:check_version:INFO:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup
2023-12-01 17:10:28,605:__init__:WARNING:group_id is None: disabling auto-commit.
2023-12-01 17:10:28,605:change_subscription:INFO:Updating subscribed topics to: ('stock_data',)
2023-12-01 17:10:28,608:assign_from_subscribed:INFO:Updated partition assignment: [TopicPartition(topic='stock_data', partition=0)]
2023-12-01 17:10:28,611:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: connecting to vl965-172-31-254-167.wireless.umass.edu:9092 [('172.31.254.167', 9092) IPv4]
2023-12-01 17:10:28,713:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: Connection complete.
2023-12-01 17:10:28,714:close:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
2023-12-01 17:10:41,947:StockPricePrediction:ERROR:An error occurred while calling o145.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3) (vl965-172-31-254-167.wireless.umass.edu executor driver): org.apache.spark.SparkException: 
Bad data in pyspark.daemon's standard output. Invalid port number:
  1231975525 (0x496e7465)
Python command to execute the daemon was:
  python3 -m pyspark.daemon
Check that you don't have any unexpected modules or libraries in
your PYTHONPATH:
  /Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip:/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip:/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/jars/spark-core_2.12-3.5.0.jar
Also, check if you have a sitecustomize.py module in your python path,
or in your python installation, that is printing to standard output
	at org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:267)
	at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)
	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3549)
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:315)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: org.apache.spark.SparkException: 
Bad data in pyspark.daemon's standard output. Invalid port number:
  1231975525 (0x496e7465)
Python command to execute the daemon was:
  python3 -m pyspark.daemon
Check that you don't have any unexpected modules or libraries in
your PYTHONPATH:
  /Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip:/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip:/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/jars/spark-core_2.12-3.5.0.jar
Also, check if you have a sitecustomize.py module in your python path,
or in your python installation, that is printing to standard output
	at org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:267)
	at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)
	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	... 1 more

2023-12-01 17:10:41,957:_log:WARNING: * Debugger is active!
2023-12-01 17:10:41,975:_log:INFO: * Debugger PIN: 514-370-077
2023-12-01 17:11:16,211:close:INFO:Closing down clientserver connection
2023-12-01 17:11:16,391:close:INFO:Closing down clientserver connection
2023-12-01 17:11:24,545:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
2023-12-01 17:11:24,545:check_version:INFO:Probing node bootstrap-0 broker version
2023-12-01 17:11:24,546:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
2023-12-01 17:11:24,654:check_version:INFO:Broker version identified as 2.5.0
2023-12-01 17:11:24,654:check_version:INFO:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup
2023-12-01 17:11:24,655:__init__:WARNING:group_id is None: disabling auto-commit.
2023-12-01 17:11:24,655:change_subscription:INFO:Updating subscribed topics to: ('stock_data',)
2023-12-01 17:11:24,657:assign_from_subscribed:INFO:Updated partition assignment: [TopicPartition(topic='stock_data', partition=0)]
2023-12-01 17:11:24,660:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: connecting to vl965-172-31-254-167.wireless.umass.edu:9092 [('172.31.254.167', 9092) IPv4]
2023-12-01 17:11:24,766:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: Connection complete.
2023-12-01 17:11:24,767:close:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
2023-12-01 17:11:28,665:send_command:INFO:Error while receiving.
Traceback (most recent call last):
  File "/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: reentrant call inside <_io.BufferedReader name=4>
2023-12-01 17:11:28,668:close:INFO:Closing down clientserver connection
2023-12-01 17:11:28,668:send_command:ERROR:Exception while sending command.
Traceback (most recent call last):
  File "/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: reentrant call inside <_io.BufferedReader name=4>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2023-12-01 17:11:28,673:send_command:INFO:Error while receiving.
Traceback (most recent call last):
  File "/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/socket.py", line 706, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/context.py", line 381, in signal_handler
    self.cancelAllJobs()
  File "/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/context.py", line 2446, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
    ^^^^^^^^^^^^^^
  File "/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o13.sc
2023-12-01 17:11:28,675:close:INFO:Closing down clientserver connection
2023-12-01 17:11:28,675:send_command:ERROR:Exception while sending command.
Traceback (most recent call last):
  File "/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/socket.py", line 706, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/context.py", line 381, in signal_handler
    self.cancelAllJobs()
  File "/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/context.py", line 2446, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
    ^^^^^^^^^^^^^^
  File "/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o13.sc

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2023-12-01 17:11:28,676:close:INFO:Closing down clientserver connection
2023-12-01 17:11:28,677:StockPricePrediction:ERROR:An error occurred while calling o145.showString
2023-12-01 17:11:28,691:_log:INFO:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:8080
 * Running on http://172.31.254.167:8080
2023-12-01 17:11:28,691:_log:INFO:[33mPress CTRL+C to quit[0m
2023-12-01 17:11:28,692:_log:INFO: * Restarting with stat
2023-12-01 17:11:29,043:close:INFO:Closing down clientserver connection
2023-12-01 17:11:37,968:_log:INFO:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:8080
 * Running on http://172.31.254.167:8080
2023-12-01 17:11:37,968:_log:INFO:[33mPress CTRL+C to quit[0m
2023-12-01 17:11:37,971:_log:INFO: * Restarting with stat
2023-12-01 17:11:38,083:_log:WARNING: * Debugger is active!
2023-12-01 17:11:38,096:_log:INFO: * Debugger PIN: 514-370-077
2023-12-01 17:12:19,406:_log:INFO: * Detected change in '/Users/mnswdhw/Documents/Courses/CS532/Stock_Data_Prediction/Stock_Data_Prediction_Of_Live_Data/app.py', reloading
2023-12-01 17:12:19,444:_log:INFO: * Restarting with stat
2023-12-01 17:12:19,571:_log:WARNING: * Debugger is active!
2023-12-01 17:12:19,582:_log:INFO: * Debugger PIN: 514-370-077
2023-12-01 17:12:20,596:_log:INFO: * Detected change in '/Users/mnswdhw/Documents/Courses/CS532/Stock_Data_Prediction/Stock_Data_Prediction_Of_Live_Data/app.py', reloading
2023-12-01 17:12:20,632:_log:INFO: * Restarting with stat
2023-12-01 17:12:20,759:_log:WARNING: * Debugger is active!
2023-12-01 17:12:20,770:_log:INFO: * Debugger PIN: 514-370-077
2023-12-01 17:12:26,806:_log:INFO:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:8080
2023-12-01 17:12:26,806:_log:INFO:[33mPress CTRL+C to quit[0m
2023-12-01 17:12:26,809:_log:INFO: * Restarting with stat
2023-12-01 17:12:26,921:_log:WARNING: * Debugger is active!
2023-12-01 17:12:26,932:_log:INFO: * Debugger PIN: 514-370-077
2023-12-01 17:12:44,548:_log:INFO:127.0.0.1 - - [01/Dec/2023 17:12:44] "GET / HTTP/1.1" 200 -
2023-12-01 17:12:44,959:_log:INFO:127.0.0.1 - - [01/Dec/2023 17:12:44] "[33mGET /data HTTP/1.1[0m" 404 -
2023-12-01 17:12:44,969:_log:INFO:127.0.0.1 - - [01/Dec/2023 17:12:44] "[33mGET /favicon.ico HTTP/1.1[0m" 404 -
2023-12-01 17:13:18,252:_log:INFO: * Detected change in '/Users/mnswdhw/Documents/Courses/CS532/Stock_Data_Prediction/Stock_Data_Prediction_Of_Live_Data/app.py', reloading
2023-12-01 17:13:18,288:_log:INFO: * Restarting with stat
2023-12-01 17:13:22,869:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
2023-12-01 17:13:22,870:check_version:INFO:Probing node bootstrap-0 broker version
2023-12-01 17:13:22,870:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
2023-12-01 17:13:22,979:check_version:INFO:Broker version identified as 2.5.0
2023-12-01 17:13:22,979:check_version:INFO:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup
2023-12-01 17:13:22,979:__init__:WARNING:group_id is None: disabling auto-commit.
2023-12-01 17:13:22,979:change_subscription:INFO:Updating subscribed topics to: ('stock_data',)
2023-12-01 17:13:22,981:assign_from_subscribed:INFO:Updated partition assignment: [TopicPartition(topic='stock_data', partition=0)]
2023-12-01 17:13:22,984:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: connecting to vl965-172-31-254-167.wireless.umass.edu:9092 [('172.31.254.167', 9092) IPv4]
2023-12-01 17:13:23,086:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: Connection complete.
2023-12-01 17:13:23,087:close:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
2023-12-01 17:13:36,181:StockPricePrediction:ERROR:An error occurred while calling o145.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3) (vl965-172-31-254-167.wireless.umass.edu executor driver): org.apache.spark.SparkException: 
Bad data in pyspark.daemon's standard output. Invalid port number:
  1231975525 (0x496e7465)
Python command to execute the daemon was:
  python3 -m pyspark.daemon
Check that you don't have any unexpected modules or libraries in
your PYTHONPATH:
  /Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip:/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip:/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/jars/spark-core_2.12-3.5.0.jar
Also, check if you have a sitecustomize.py module in your python path,
or in your python installation, that is printing to standard output
	at org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:267)
	at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)
	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3549)
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:315)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: org.apache.spark.SparkException: 
Bad data in pyspark.daemon's standard output. Invalid port number:
  1231975525 (0x496e7465)
Python command to execute the daemon was:
  python3 -m pyspark.daemon
Check that you don't have any unexpected modules or libraries in
your PYTHONPATH:
  /Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip:/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip:/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/jars/spark-core_2.12-3.5.0.jar
Also, check if you have a sitecustomize.py module in your python path,
or in your python installation, that is printing to standard output
	at org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:267)
	at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)
	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	... 1 more

2023-12-01 17:13:36,191:_log:WARNING: * Debugger is active!
2023-12-01 17:13:36,209:_log:INFO: * Debugger PIN: 514-370-077
2023-12-01 17:14:01,417:_log:INFO: * Detected change in '/Users/mnswdhw/Documents/Courses/CS532/Stock_Data_Prediction/Stock_Data_Prediction_Of_Live_Data/app.py', reloading
2023-12-01 17:14:01,419:close:INFO:Closing down clientserver connection
2023-12-01 17:14:01,552:_log:INFO: * Restarting with stat
2023-12-01 17:14:06,379:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
2023-12-01 17:14:06,379:check_version:INFO:Probing node bootstrap-0 broker version
2023-12-01 17:14:06,380:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
2023-12-01 17:14:06,488:check_version:INFO:Broker version identified as 2.5.0
2023-12-01 17:14:06,488:check_version:INFO:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup
2023-12-01 17:14:06,489:__init__:WARNING:group_id is None: disabling auto-commit.
2023-12-01 17:14:06,489:change_subscription:INFO:Updating subscribed topics to: ('stock_data',)
2023-12-01 17:14:06,491:assign_from_subscribed:INFO:Updated partition assignment: [TopicPartition(topic='stock_data', partition=0)]
2023-12-01 17:14:06,494:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: connecting to vl965-172-31-254-167.wireless.umass.edu:9092 [('172.31.254.167', 9092) IPv4]
2023-12-01 17:14:06,597:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: Connection complete.
2023-12-01 17:14:06,597:close:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
2023-12-01 17:14:18,229:StockPricePrediction:ERROR:An error occurred while calling o145.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3) (vl965-172-31-254-167.wireless.umass.edu executor driver): org.apache.spark.SparkException: 
Bad data in pyspark.daemon's standard output. Invalid port number:
  1231975525 (0x496e7465)
Python command to execute the daemon was:
  python3 -m pyspark.daemon
Check that you don't have any unexpected modules or libraries in
your PYTHONPATH:
  /Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip:/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip:/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/jars/spark-core_2.12-3.5.0.jar
Also, check if you have a sitecustomize.py module in your python path,
or in your python installation, that is printing to standard output
	at org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:267)
	at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)
	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3549)
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:315)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: org.apache.spark.SparkException: 
Bad data in pyspark.daemon's standard output. Invalid port number:
  1231975525 (0x496e7465)
Python command to execute the daemon was:
  python3 -m pyspark.daemon
Check that you don't have any unexpected modules or libraries in
your PYTHONPATH:
  /Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip:/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip:/Users/mnswdhw/miniconda/envs/playground/lib/python3.11/site-packages/pyspark/jars/spark-core_2.12-3.5.0.jar
Also, check if you have a sitecustomize.py module in your python path,
or in your python installation, that is printing to standard output
	at org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:267)
	at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)
	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	... 1 more

2023-12-01 17:14:18,239:_log:WARNING: * Debugger is active!
2023-12-01 17:14:18,254:_log:INFO: * Debugger PIN: 514-370-077
2023-12-01 17:16:31,402:_log:INFO: * Detected change in '/Users/mnswdhw/Documents/Courses/CS532/Stock_Data_Prediction/Stock_Data_Prediction_Of_Live_Data/Consumer.py', reloading
2023-12-01 17:16:31,404:close:INFO:Closing down clientserver connection
2023-12-01 17:16:31,532:_log:INFO: * Restarting with stat
2023-12-01 17:16:34,048:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
2023-12-01 17:16:34,048:check_version:INFO:Probing node bootstrap-0 broker version
2023-12-01 17:16:34,049:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
2023-12-01 17:16:34,155:check_version:INFO:Broker version identified as 2.5.0
2023-12-01 17:16:34,155:check_version:INFO:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup
2023-12-01 17:16:34,156:__init__:WARNING:group_id is None: disabling auto-commit.
2023-12-01 17:16:34,156:change_subscription:INFO:Updating subscribed topics to: ('stock_data',)
2023-12-01 17:16:34,158:assign_from_subscribed:INFO:Updated partition assignment: [TopicPartition(topic='stock_data', partition=0)]
2023-12-01 17:16:34,161:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: connecting to vl965-172-31-254-167.wireless.umass.edu:9092 [('172.31.254.167', 9092) IPv4]
2023-12-01 17:16:34,267:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: Connection complete.
2023-12-01 17:16:34,267:close:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
2023-12-01 17:16:35,661:_log:WARNING: * Debugger is active!
2023-12-01 17:16:35,676:_log:INFO: * Debugger PIN: 514-370-077
2023-12-01 17:16:53,836:_log:INFO: * Detected change in '/Users/mnswdhw/Documents/Courses/CS532/Stock_Data_Prediction/Stock_Data_Prediction_Of_Live_Data/app.py', reloading
2023-12-01 17:16:53,839:close:INFO:Closing down clientserver connection
2023-12-01 17:16:53,953:_log:INFO: * Restarting with stat
2023-12-01 17:16:56,458:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
2023-12-01 17:16:56,458:check_version:INFO:Probing node bootstrap-0 broker version
2023-12-01 17:16:56,459:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
2023-12-01 17:16:56,567:check_version:INFO:Broker version identified as 2.5.0
2023-12-01 17:16:56,567:check_version:INFO:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup
2023-12-01 17:16:56,568:__init__:WARNING:group_id is None: disabling auto-commit.
2023-12-01 17:16:56,568:change_subscription:INFO:Updating subscribed topics to: ('stock_data',)
2023-12-01 17:16:56,570:assign_from_subscribed:INFO:Updated partition assignment: [TopicPartition(topic='stock_data', partition=0)]
2023-12-01 17:16:56,573:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: connecting to vl965-172-31-254-167.wireless.umass.edu:9092 [('172.31.254.167', 9092) IPv4]
2023-12-01 17:16:56,677:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: Connection complete.
2023-12-01 17:16:56,677:close:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
2023-12-01 17:16:59,750:_log:WARNING: * Debugger is active!
2023-12-01 17:16:59,765:_log:INFO: * Debugger PIN: 514-370-077
2023-12-01 17:18:04,335:_log:INFO: * Detected change in '/Users/mnswdhw/Documents/Courses/CS532/Stock_Data_Prediction/Stock_Data_Prediction_Of_Live_Data/app.py', reloading
2023-12-01 17:18:04,337:close:INFO:Closing down clientserver connection
2023-12-01 17:18:04,471:_log:INFO: * Restarting with stat
2023-12-01 17:18:12,516:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
2023-12-01 17:18:12,516:check_version:INFO:Probing node bootstrap-0 broker version
2023-12-01 17:18:12,516:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
2023-12-01 17:18:12,621:check_version:INFO:Broker version identified as 2.5.0
2023-12-01 17:18:12,622:check_version:INFO:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup
2023-12-01 17:18:12,622:__init__:WARNING:group_id is None: disabling auto-commit.
2023-12-01 17:18:12,622:change_subscription:INFO:Updating subscribed topics to: ('stock_data',)
2023-12-01 17:18:12,625:assign_from_subscribed:INFO:Updated partition assignment: [TopicPartition(topic='stock_data', partition=0)]
2023-12-01 17:18:12,627:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: connecting to vl965-172-31-254-167.wireless.umass.edu:9092 [('172.31.254.167', 9092) IPv4]
2023-12-01 17:18:12,733:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: Connection complete.
2023-12-01 17:18:12,733:close:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
2023-12-01 17:18:14,827:_log:INFO:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:8080
2023-12-01 17:18:14,827:_log:INFO:[33mPress CTRL+C to quit[0m
2023-12-01 17:18:14,827:_log:INFO: * Restarting with stat
2023-12-01 17:18:17,055:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
2023-12-01 17:18:17,055:check_version:INFO:Probing node bootstrap-0 broker version
2023-12-01 17:18:17,056:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
2023-12-01 17:18:17,163:check_version:INFO:Broker version identified as 2.5.0
2023-12-01 17:18:17,165:check_version:INFO:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup
2023-12-01 17:18:17,166:__init__:WARNING:group_id is None: disabling auto-commit.
2023-12-01 17:18:17,166:change_subscription:INFO:Updating subscribed topics to: ('stock_data',)
2023-12-01 17:18:17,168:assign_from_subscribed:INFO:Updated partition assignment: [TopicPartition(topic='stock_data', partition=0)]
2023-12-01 17:18:17,171:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: connecting to vl965-172-31-254-167.wireless.umass.edu:9092 [('172.31.254.167', 9092) IPv4]
2023-12-01 17:18:17,273:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: Connection complete.
2023-12-01 17:18:17,274:close:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
2023-12-01 17:18:20,842:_log:WARNING: * Debugger is active!
2023-12-01 17:18:20,858:_log:INFO: * Debugger PIN: 514-370-077
2023-12-01 17:18:23,680:_log:INFO:127.0.0.1 - - [01/Dec/2023 17:18:23] "GET / HTTP/1.1" 200 -
2023-12-01 17:18:23,773:data:INFO:module 'Consumer' has no attribute 'LoadModel'
2023-12-01 17:18:23,776:data:INFO:module 'Consumer' has no attribute 'LoadModel'
2023-12-01 17:18:23,777:data:INFO:module 'Consumer' has no attribute 'LoadModel'
2023-12-01 17:18:29,781:data:INFO:module 'Consumer' has no attribute 'LoadModel'
2023-12-01 17:18:29,783:data:INFO:module 'Consumer' has no attribute 'LoadModel'
2023-12-01 17:18:29,785:data:INFO:module 'Consumer' has no attribute 'LoadModel'
2023-12-01 17:18:44,070:_log:INFO: * Detected change in '/Users/mnswdhw/Documents/Courses/CS532/Stock_Data_Prediction/Stock_Data_Prediction_Of_Live_Data/app.py', reloading
2023-12-01 17:18:44,072:close:INFO:Closing down clientserver connection
2023-12-01 17:18:44,188:_log:INFO: * Restarting with stat
2023-12-01 17:18:45,421:close:INFO:Closing down clientserver connection
2023-12-01 17:18:52,768:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
2023-12-01 17:18:52,768:check_version:INFO:Probing node bootstrap-0 broker version
2023-12-01 17:18:52,769:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
2023-12-01 17:18:52,878:check_version:INFO:Broker version identified as 2.5.0
2023-12-01 17:18:52,878:check_version:INFO:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup
2023-12-01 17:18:52,879:__init__:WARNING:group_id is None: disabling auto-commit.
2023-12-01 17:18:52,879:change_subscription:INFO:Updating subscribed topics to: ('stock_data',)
2023-12-01 17:18:52,881:assign_from_subscribed:INFO:Updated partition assignment: [TopicPartition(topic='stock_data', partition=0)]
2023-12-01 17:18:52,884:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: connecting to vl965-172-31-254-167.wireless.umass.edu:9092 [('172.31.254.167', 9092) IPv4]
2023-12-01 17:18:52,990:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: Connection complete.
2023-12-01 17:18:52,991:close:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
2023-12-01 17:18:56,880:_log:INFO:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:8080
2023-12-01 17:18:56,880:_log:INFO:[33mPress CTRL+C to quit[0m
2023-12-01 17:18:56,881:_log:INFO: * Restarting with stat
2023-12-01 17:18:59,253:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
2023-12-01 17:18:59,253:check_version:INFO:Probing node bootstrap-0 broker version
2023-12-01 17:18:59,254:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
2023-12-01 17:18:59,362:check_version:INFO:Broker version identified as 2.5.0
2023-12-01 17:18:59,362:check_version:INFO:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup
2023-12-01 17:18:59,363:__init__:WARNING:group_id is None: disabling auto-commit.
2023-12-01 17:18:59,363:change_subscription:INFO:Updating subscribed topics to: ('stock_data',)
2023-12-01 17:18:59,365:assign_from_subscribed:INFO:Updated partition assignment: [TopicPartition(topic='stock_data', partition=0)]
2023-12-01 17:18:59,368:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: connecting to vl965-172-31-254-167.wireless.umass.edu:9092 [('172.31.254.167', 9092) IPv4]
2023-12-01 17:18:59,470:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: Connection complete.
2023-12-01 17:18:59,470:close:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
2023-12-01 17:19:02,910:_log:WARNING: * Debugger is active!
2023-12-01 17:19:02,926:_log:INFO: * Debugger PIN: 514-370-077
2023-12-01 17:19:06,508:data:INFO:module 'Consumer' has no attribute 'LoadModel'
2023-12-01 17:19:06,511:data:INFO:module 'Consumer' has no attribute 'LoadModel'
2023-12-01 17:19:06,513:data:INFO:module 'Consumer' has no attribute 'LoadModel'
2023-12-01 17:19:19,960:close:INFO:Closing down clientserver connection
2023-12-01 17:19:20,139:close:INFO:Closing down clientserver connection
2023-12-01 17:19:23,673:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
2023-12-01 17:19:23,674:check_version:INFO:Probing node bootstrap-0 broker version
2023-12-01 17:19:23,674:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
2023-12-01 17:19:23,782:check_version:INFO:Broker version identified as 2.5.0
2023-12-01 17:19:23,783:check_version:INFO:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup
2023-12-01 17:19:23,783:__init__:WARNING:group_id is None: disabling auto-commit.
2023-12-01 17:19:23,783:change_subscription:INFO:Updating subscribed topics to: ('stock_data',)
2023-12-01 17:19:23,785:assign_from_subscribed:INFO:Updated partition assignment: [TopicPartition(topic='stock_data', partition=0)]
2023-12-01 17:19:23,788:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: connecting to vl965-172-31-254-167.wireless.umass.edu:9092 [('172.31.254.167', 9092) IPv4]
2023-12-01 17:19:23,894:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: Connection complete.
2023-12-01 17:19:23,894:close:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
2023-12-01 17:19:26,979:_log:INFO:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:8080
2023-12-01 17:19:26,979:_log:INFO:[33mPress CTRL+C to quit[0m
2023-12-01 17:19:26,979:_log:INFO: * Restarting with stat
2023-12-01 17:19:29,327:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
2023-12-01 17:19:29,327:check_version:INFO:Probing node bootstrap-0 broker version
2023-12-01 17:19:29,328:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
2023-12-01 17:19:29,435:check_version:INFO:Broker version identified as 2.5.0
2023-12-01 17:19:29,435:check_version:INFO:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup
2023-12-01 17:19:29,436:__init__:WARNING:group_id is None: disabling auto-commit.
2023-12-01 17:19:29,436:change_subscription:INFO:Updating subscribed topics to: ('stock_data',)
2023-12-01 17:19:29,437:assign_from_subscribed:INFO:Updated partition assignment: [TopicPartition(topic='stock_data', partition=0)]
2023-12-01 17:19:29,440:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: connecting to vl965-172-31-254-167.wireless.umass.edu:9092 [('172.31.254.167', 9092) IPv4]
2023-12-01 17:19:29,546:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: Connection complete.
2023-12-01 17:19:29,546:close:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
2023-12-01 17:19:32,915:_log:WARNING: * Debugger is active!
2023-12-01 17:19:32,932:_log:INFO: * Debugger PIN: 514-370-077
2023-12-01 17:19:32,938:data:INFO:module 'Consumer' has no attribute 'LoadModel'
2023-12-01 17:19:32,940:data:INFO:module 'Consumer' has no attribute 'LoadModel'
2023-12-01 17:19:32,941:data:INFO:module 'Consumer' has no attribute 'LoadModel'
2023-12-01 17:20:00,186:_log:INFO: * Detected change in '/Users/mnswdhw/Documents/Courses/CS532/Stock_Data_Prediction/Stock_Data_Prediction_Of_Live_Data/app.py', reloading
2023-12-01 17:20:00,189:close:INFO:Closing down clientserver connection
2023-12-01 17:20:00,305:_log:INFO: * Restarting with stat
2023-12-01 17:20:00,374:close:INFO:Closing down clientserver connection
2023-12-01 17:20:06,128:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
2023-12-01 17:20:06,128:check_version:INFO:Probing node bootstrap-0 broker version
2023-12-01 17:20:06,129:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
2023-12-01 17:20:06,237:check_version:INFO:Broker version identified as 2.5.0
2023-12-01 17:20:06,237:check_version:INFO:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup
2023-12-01 17:20:06,238:__init__:WARNING:group_id is None: disabling auto-commit.
2023-12-01 17:20:06,238:change_subscription:INFO:Updating subscribed topics to: ('stock_data',)
2023-12-01 17:20:06,240:assign_from_subscribed:INFO:Updated partition assignment: [TopicPartition(topic='stock_data', partition=0)]
2023-12-01 17:20:06,243:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: connecting to vl965-172-31-254-167.wireless.umass.edu:9092 [('172.31.254.167', 9092) IPv4]
2023-12-01 17:20:06,348:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: Connection complete.
2023-12-01 17:20:06,349:close:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
2023-12-01 17:20:08,950:_log:INFO:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:8080
2023-12-01 17:20:08,950:_log:INFO:[33mPress CTRL+C to quit[0m
2023-12-01 17:20:08,950:_log:INFO: * Restarting with stat
2023-12-01 17:20:11,162:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
2023-12-01 17:20:11,163:check_version:INFO:Probing node bootstrap-0 broker version
2023-12-01 17:20:11,163:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
2023-12-01 17:20:11,268:check_version:INFO:Broker version identified as 2.5.0
2023-12-01 17:20:11,268:check_version:INFO:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup
2023-12-01 17:20:11,269:__init__:WARNING:group_id is None: disabling auto-commit.
2023-12-01 17:20:11,269:change_subscription:INFO:Updating subscribed topics to: ('stock_data',)
2023-12-01 17:20:11,271:assign_from_subscribed:INFO:Updated partition assignment: [TopicPartition(topic='stock_data', partition=0)]
2023-12-01 17:20:11,275:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: connecting to vl965-172-31-254-167.wireless.umass.edu:9092 [('172.31.254.167', 9092) IPv4]
2023-12-01 17:20:11,378:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: Connection complete.
2023-12-01 17:20:11,378:close:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
2023-12-01 17:20:14,979:_log:WARNING: * Debugger is active!
2023-12-01 17:20:14,997:_log:INFO: * Debugger PIN: 514-370-077
2023-12-01 17:20:18,907:_log:INFO:127.0.0.1 - - [01/Dec/2023 17:20:18] "GET /data HTTP/1.1" 200 -
2023-12-01 17:20:18,908:close:INFO:Closing down clientserver connection
2023-12-01 17:20:53,352:_log:INFO: * Detected change in '/Users/mnswdhw/Documents/Courses/CS532/Stock_Data_Prediction/Stock_Data_Prediction_Of_Live_Data/Consumer.py', reloading
2023-12-01 17:20:53,353:close:INFO:Closing down clientserver connection
2023-12-01 17:20:53,471:_log:INFO: * Restarting with stat
2023-12-01 17:20:56,049:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
2023-12-01 17:20:56,049:check_version:INFO:Probing node bootstrap-0 broker version
2023-12-01 17:20:56,050:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
2023-12-01 17:20:56,157:check_version:INFO:Broker version identified as 2.5.0
2023-12-01 17:20:56,157:check_version:INFO:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup
2023-12-01 17:20:56,158:__init__:WARNING:group_id is None: disabling auto-commit.
2023-12-01 17:20:56,158:change_subscription:INFO:Updating subscribed topics to: ('stock_data',)
2023-12-01 17:20:56,160:assign_from_subscribed:INFO:Updated partition assignment: [TopicPartition(topic='stock_data', partition=0)]
2023-12-01 17:20:56,162:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: connecting to vl965-172-31-254-167.wireless.umass.edu:9092 [('172.31.254.167', 9092) IPv4]
2023-12-01 17:20:56,266:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: Connection complete.
2023-12-01 17:20:56,266:close:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
2023-12-01 17:21:00,029:_log:WARNING: * Debugger is active!
2023-12-01 17:21:00,045:_log:INFO: * Debugger PIN: 514-370-077
2023-12-01 17:21:01,062:_log:INFO: * Detected change in '/Users/mnswdhw/Documents/Courses/CS532/Stock_Data_Prediction/Stock_Data_Prediction_Of_Live_Data/Consumer.py', reloading
2023-12-01 17:21:01,062:close:INFO:Closing down clientserver connection
2023-12-01 17:21:01,184:_log:INFO: * Restarting with stat
2023-12-01 17:21:03,668:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
2023-12-01 17:21:03,668:check_version:INFO:Probing node bootstrap-0 broker version
2023-12-01 17:21:03,669:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
2023-12-01 17:21:03,776:check_version:INFO:Broker version identified as 2.5.0
2023-12-01 17:21:03,776:check_version:INFO:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup
2023-12-01 17:21:03,777:__init__:WARNING:group_id is None: disabling auto-commit.
2023-12-01 17:21:03,777:change_subscription:INFO:Updating subscribed topics to: ('stock_data',)
2023-12-01 17:21:03,778:assign_from_subscribed:INFO:Updated partition assignment: [TopicPartition(topic='stock_data', partition=0)]
2023-12-01 17:21:03,781:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: connecting to vl965-172-31-254-167.wireless.umass.edu:9092 [('172.31.254.167', 9092) IPv4]
2023-12-01 17:21:03,882:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: Connection complete.
2023-12-01 17:21:03,882:close:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
2023-12-01 17:21:04,076:close:INFO:Closing down clientserver connection
2023-12-01 17:21:10,963:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
2023-12-01 17:21:10,963:check_version:INFO:Probing node bootstrap-0 broker version
2023-12-01 17:21:10,963:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
2023-12-01 17:21:11,072:check_version:INFO:Broker version identified as 2.5.0
2023-12-01 17:21:11,072:check_version:INFO:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup
2023-12-01 17:21:11,072:__init__:WARNING:group_id is None: disabling auto-commit.
2023-12-01 17:21:11,072:change_subscription:INFO:Updating subscribed topics to: ('stock_data',)
2023-12-01 17:21:11,074:assign_from_subscribed:INFO:Updated partition assignment: [TopicPartition(topic='stock_data', partition=0)]
2023-12-01 17:21:11,077:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: connecting to vl965-172-31-254-167.wireless.umass.edu:9092 [('172.31.254.167', 9092) IPv4]
2023-12-01 17:21:11,182:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: Connection complete.
2023-12-01 17:21:11,183:close:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
2023-12-01 17:21:15,025:_log:INFO:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:8080
2023-12-01 17:21:15,025:_log:INFO:[33mPress CTRL+C to quit[0m
2023-12-01 17:21:15,026:_log:INFO: * Restarting with stat
2023-12-01 17:21:17,239:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
2023-12-01 17:21:17,239:check_version:INFO:Probing node bootstrap-0 broker version
2023-12-01 17:21:17,239:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
2023-12-01 17:21:17,347:check_version:INFO:Broker version identified as 2.5.0
2023-12-01 17:21:17,347:check_version:INFO:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup
2023-12-01 17:21:17,347:__init__:WARNING:group_id is None: disabling auto-commit.
2023-12-01 17:21:17,348:change_subscription:INFO:Updating subscribed topics to: ('stock_data',)
2023-12-01 17:21:17,350:assign_from_subscribed:INFO:Updated partition assignment: [TopicPartition(topic='stock_data', partition=0)]
2023-12-01 17:21:17,353:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: connecting to vl965-172-31-254-167.wireless.umass.edu:9092 [('172.31.254.167', 9092) IPv4]
2023-12-01 17:21:17,456:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: Connection complete.
2023-12-01 17:21:17,457:close:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
2023-12-01 17:21:21,089:_log:WARNING: * Debugger is active!
2023-12-01 17:21:21,106:_log:INFO: * Debugger PIN: 514-370-077
2023-12-01 17:21:24,959:_log:INFO:127.0.0.1 - - [01/Dec/2023 17:21:24] "GET /data HTTP/1.1" 200 -
2023-12-01 17:21:24,961:close:INFO:Closing down clientserver connection
2023-12-01 17:23:34,931:close:INFO:Closing the Kafka producer with 0 secs timeout.
2023-12-01 17:23:34,932:close:INFO:Proceeding to force close the producer since pending requests could not be completed within timeout 0.
2023-12-01 17:23:49,345:_log:INFO: * Detected change in '/Users/mnswdhw/Documents/Courses/CS532/Stock_Data_Prediction/Stock_Data_Prediction_Of_Live_Data/producer.py', reloading
2023-12-01 17:23:49,345:close:INFO:Closing down clientserver connection
2023-12-01 17:23:49,456:_log:INFO: * Restarting with stat
2023-12-01 17:23:52,021:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
2023-12-01 17:23:52,021:check_version:INFO:Probing node bootstrap-0 broker version
2023-12-01 17:23:52,022:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
2023-12-01 17:23:52,130:check_version:INFO:Broker version identified as 2.5.0
2023-12-01 17:23:52,130:check_version:INFO:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup
2023-12-01 17:23:52,130:__init__:WARNING:group_id is None: disabling auto-commit.
2023-12-01 17:23:52,130:change_subscription:INFO:Updating subscribed topics to: ('stock_data',)
2023-12-01 17:23:52,132:assign_from_subscribed:INFO:Updated partition assignment: [TopicPartition(topic='stock_data', partition=0)]
2023-12-01 17:23:52,135:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: connecting to vl965-172-31-254-167.wireless.umass.edu:9092 [('172.31.254.167', 9092) IPv4]
2023-12-01 17:23:52,239:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: Connection complete.
2023-12-01 17:23:52,239:close:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
2023-12-01 17:23:53,512:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
2023-12-01 17:23:53,512:check_version:INFO:Probing node bootstrap-0 broker version
2023-12-01 17:23:53,512:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
2023-12-01 17:23:53,616:check_version:INFO:Broker version identified as 2.5.0
2023-12-01 17:23:53,616:check_version:INFO:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup
2023-12-01 17:23:53,621:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: connecting to vl965-172-31-254-167.wireless.umass.edu:9092 [('172.31.254.167', 9092) IPv4]
2023-12-01 17:23:53,622:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: Connection complete.
2023-12-01 17:23:53,622:close:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
2023-12-01 17:23:54,796:_log:WARNING: * Debugger is active!
2023-12-01 17:23:54,811:_log:INFO: * Debugger PIN: 514-370-077
2023-12-01 17:25:02,768:close:INFO:Closing down clientserver connection
2023-12-01 17:25:02,944:close:INFO:Closing down clientserver connection
2023-12-01 17:25:10,380:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
2023-12-01 17:25:10,380:check_version:INFO:Probing node bootstrap-0 broker version
2023-12-01 17:25:10,381:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
2023-12-01 17:25:10,489:check_version:INFO:Broker version identified as 2.5.0
2023-12-01 17:25:10,490:check_version:INFO:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup
2023-12-01 17:25:10,490:__init__:WARNING:group_id is None: disabling auto-commit.
2023-12-01 17:25:10,490:change_subscription:INFO:Updating subscribed topics to: ('stock_data',)
2023-12-01 17:25:10,492:assign_from_subscribed:INFO:Updated partition assignment: [TopicPartition(topic='stock_data', partition=0)]
2023-12-01 17:25:10,495:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: connecting to vl965-172-31-254-167.wireless.umass.edu:9092 [('172.31.254.167', 9092) IPv4]
2023-12-01 17:25:10,598:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: Connection complete.
2023-12-01 17:25:10,599:close:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
2023-12-01 17:25:12,909:_log:INFO:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:8080
2023-12-01 17:25:12,909:_log:INFO:[33mPress CTRL+C to quit[0m
2023-12-01 17:25:12,909:_log:INFO: * Restarting with stat
2023-12-01 17:25:15,134:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
2023-12-01 17:25:15,134:check_version:INFO:Probing node bootstrap-0 broker version
2023-12-01 17:25:15,134:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
2023-12-01 17:25:15,238:check_version:INFO:Broker version identified as 2.5.0
2023-12-01 17:25:15,238:check_version:INFO:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup
2023-12-01 17:25:15,238:__init__:WARNING:group_id is None: disabling auto-commit.
2023-12-01 17:25:15,239:change_subscription:INFO:Updating subscribed topics to: ('stock_data',)
2023-12-01 17:25:15,240:assign_from_subscribed:INFO:Updated partition assignment: [TopicPartition(topic='stock_data', partition=0)]
2023-12-01 17:25:15,243:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: connecting to vl965-172-31-254-167.wireless.umass.edu:9092 [('172.31.254.167', 9092) IPv4]
2023-12-01 17:25:15,347:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: Connection complete.
2023-12-01 17:25:15,347:close:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
2023-12-01 17:25:18,905:_log:WARNING: * Debugger is active!
2023-12-01 17:25:18,923:_log:INFO: * Debugger PIN: 514-370-077
2023-12-01 17:25:44,670:_log:INFO:127.0.0.1 - - [01/Dec/2023 17:25:44] "GET /data HTTP/1.1" 200 -
2023-12-01 17:25:44,672:close:INFO:Closing down clientserver connection
2023-12-01 17:26:01,056:_log:INFO:127.0.0.1 - - [01/Dec/2023 17:26:01] "GET /data HTTP/1.1" 200 -
2023-12-01 17:26:01,058:close:INFO:Closing down clientserver connection
2023-12-01 17:26:19,340:_log:INFO:127.0.0.1 - - [01/Dec/2023 17:26:19] "GET /data HTTP/1.1" 200 -
2023-12-01 17:26:19,342:close:INFO:Closing down clientserver connection
2023-12-01 17:26:25,407:_log:INFO:127.0.0.1 - - [01/Dec/2023 17:26:25] "GET /data HTTP/1.1" 200 -
2023-12-01 17:26:25,408:close:INFO:Closing down clientserver connection
2023-12-01 17:26:37,621:_log:INFO:127.0.0.1 - - [01/Dec/2023 17:26:37] "GET /data HTTP/1.1" 200 -
2023-12-01 17:26:37,622:close:INFO:Closing down clientserver connection
2023-12-01 17:27:44,488:close:INFO:Closing down clientserver connection
2023-12-01 17:27:44,617:close:INFO:Closing down clientserver connection
2023-12-01 17:27:54,322:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
2023-12-01 17:27:54,322:check_version:INFO:Probing node bootstrap-0 broker version
2023-12-01 17:27:54,323:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
2023-12-01 17:27:54,431:check_version:INFO:Broker version identified as 2.5.0
2023-12-01 17:27:54,431:check_version:INFO:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup
2023-12-01 17:27:54,432:__init__:WARNING:group_id is None: disabling auto-commit.
2023-12-01 17:27:54,432:change_subscription:INFO:Updating subscribed topics to: ('stock_data',)
2023-12-01 17:27:54,433:assign_from_subscribed:INFO:Updated partition assignment: [TopicPartition(topic='stock_data', partition=0)]
2023-12-01 17:27:54,436:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: connecting to vl965-172-31-254-167.wireless.umass.edu:9092 [('172.31.254.167', 9092) IPv4]
2023-12-01 17:27:54,542:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: Connection complete.
2023-12-01 17:27:54,543:close:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
2023-12-01 17:27:58,150:close:INFO:Closing down clientserver connection
2023-12-01 17:28:26,239:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
2023-12-01 17:28:26,240:check_version:INFO:Probing node bootstrap-0 broker version
2023-12-01 17:28:26,240:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
2023-12-01 17:28:26,347:check_version:INFO:Broker version identified as 2.5.0
2023-12-01 17:28:26,347:check_version:INFO:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup
2023-12-01 17:28:26,348:__init__:WARNING:group_id is None: disabling auto-commit.
2023-12-01 17:28:26,348:change_subscription:INFO:Updating subscribed topics to: ('stock_data',)
2023-12-01 17:28:26,349:assign_from_subscribed:INFO:Updated partition assignment: [TopicPartition(topic='stock_data', partition=0)]
2023-12-01 17:28:26,352:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: connecting to vl965-172-31-254-167.wireless.umass.edu:9092 [('172.31.254.167', 9092) IPv4]
2023-12-01 17:28:26,455:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: Connection complete.
2023-12-01 17:28:26,455:close:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
2023-12-01 17:28:28,172:close:INFO:Closing down clientserver connection
2023-12-01 17:30:37,831:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
2023-12-01 17:30:37,832:check_version:INFO:Probing node bootstrap-0 broker version
2023-12-01 17:30:37,832:connect:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
2023-12-01 17:30:37,939:check_version:INFO:Broker version identified as 2.5.0
2023-12-01 17:30:37,939:check_version:INFO:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup
2023-12-01 17:30:37,940:__init__:WARNING:group_id is None: disabling auto-commit.
2023-12-01 17:30:37,940:change_subscription:INFO:Updating subscribed topics to: ('stock_data',)
2023-12-01 17:30:37,942:assign_from_subscribed:INFO:Updated partition assignment: [TopicPartition(topic='stock_data', partition=0)]
2023-12-01 17:30:37,944:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: connecting to vl965-172-31-254-167.wireless.umass.edu:9092 [('172.31.254.167', 9092) IPv4]
2023-12-01 17:30:38,052:connect:INFO:<BrokerConnection node_id=0 host=vl965-172-31-254-167.wireless.umass.edu:9092 <connecting> [IPv4 ('172.31.254.167', 9092)]>: Connection complete.
2023-12-01 17:30:38,052:close:INFO:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
2023-12-01 17:31:48,068:close:INFO:Closing down clientserver connection
2023-12-01 17:53:33,714:close:INFO:Closing the Kafka producer with 0 secs timeout.
2023-12-01 17:53:33,716:close:INFO:Proceeding to force close the producer since pending requests could not be completed within timeout 0.
