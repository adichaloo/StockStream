{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6e675164-b036-423c-a814-f4e57b33a577",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, FloatType\n",
    "from pyspark.sql.functions import mean, col, to_date\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc675964-9b34-447c-8d7a-7622cac7c520",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"SockStream1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e866070e-8db2-4476-9c2a-85820db0ec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_data = []\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", StringType()),\n",
    "    StructField(\"open\", FloatType()),\n",
    "    StructField(\"high\", FloatType()),\n",
    "    StructField(\"low\", FloatType()),\n",
    "    StructField(\"close\", FloatType()),\n",
    "    StructField(\"volume\", FloatType())\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5fc4f136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'timestamp': '2023-11-17 11:45:00', 'open': 153.083, 'high': 153.09, 'low': 153.005, 'close': 153.075, 'volume': 23015.0}\n",
      "+---------+-------+-------+-------+-------+-------+----+\n",
      "|timestamp|   open|   high|    low|  close| volume|date|\n",
      "+---------+-------+-------+-------+-------+-------+----+\n",
      "|     NULL|153.083| 153.09|153.005|153.075|23015.0|NULL|\n",
      "|     NULL|  153.1|153.105| 153.01|153.085|33288.0|NULL|\n",
      "|     NULL| 152.85| 153.15| 152.81| 153.11|25972.0|NULL|\n",
      "|     NULL| 152.98| 152.98|  152.8| 152.85|21931.0|NULL|\n",
      "|     NULL|  153.0| 153.03| 152.95|  153.0|27809.0|NULL|\n",
      "+---------+-------+-------+-------+-------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "listt = [\n",
    "    {\n",
    "        '2023-11-17 11:45:00': {\n",
    "            '1. open': '153.0830',\n",
    "            '2. high': '153.0900',\n",
    "            '3. low': '153.0050',\n",
    "            '4. close': '153.0750',\n",
    "            '5. volume': '23015'\n",
    "        },\n",
    "        '2023-11-17 11:40:00': {\n",
    "            '1. open': '153.1000',\n",
    "            '2. high': '153.1050',\n",
    "            '3. low': '153.0100',\n",
    "            '4. close': '153.0850',\n",
    "            '5. volume': '33288'\n",
    "        },\n",
    "        '2023-11-17 11:35:00': {\n",
    "            '1. open': '152.8500', \n",
    "            '2. high': '153.1500', \n",
    "            '3. low': '152.8100', \n",
    "            '4. close': '153.1100', \n",
    "            '5. volume': '25972'\n",
    "        },\n",
    "        '2023-11-17 11:30:00': {\n",
    "            '1. open': '152.9800', \n",
    "            '2. high': '152.9800', \n",
    "            '3. low': '152.8000', \n",
    "            '4. close': '152.8500', \n",
    "            '5. volume': '21931'\n",
    "        },\n",
    "        '2023-11-17 11:25:00': {'1. open': '153.0000', '2. high': '153.0300', '3. low': '152.9500', '4. close': '153.0000', '5. volume': '27809'},\n",
    "        '2023-11-17 11:45:00': {'1. open': '153.0830', '2. high': '153.0900', '3. low': '153.0050', '4. close': '153.0750', '5. volume': '23015'}\n",
    "    }\n",
    "]\n",
    "rows = []\n",
    "for timestamp, values in listt[0].items():\n",
    "    row_values = {\n",
    "        'timestamp': timestamp,\n",
    "        'open': float(values['1. open']),\n",
    "        'high': float(values['2. high']),\n",
    "        'low': float(values['3. low']),\n",
    "        'close': float(values['4. close']),\n",
    "        'volume': float(values['5. volume'])\n",
    "        \n",
    "    }\n",
    "    rows.append(row_values)\n",
    "print(rows[0])\n",
    "df = spark.createDataFrame(rows, schema = schema)    \n",
    "    \n",
    "for column in df.columns:\n",
    "    df = df.withColumn(column, col(column).cast(\"float\"))\n",
    "\n",
    "df = df.withColumn(\"date\", to_date(col(\"timestamp\"), 'yyyy-MM-dd HH:mm:ss'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "38fda1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+-------+--------------------+--------------------+\n",
      "|   open|   high|    low|  close| volume|intra_day_volatility|        daily_return|\n",
      "+-------+-------+-------+-------+-------+--------------------+--------------------+\n",
      "|153.083| 153.09|153.005|153.075|23015.0|         0.084991455|-5.22305276070733...|\n",
      "|  153.1|153.105| 153.01|153.085|33288.0|          0.09500122|-9.79711890951588...|\n",
      "| 152.85| 153.15| 152.81| 153.11|25972.0|          0.33999634|0.001700978059888...|\n",
      "| 152.98| 152.98|  152.8| 152.85|21931.0|          0.17999268|-8.49716483552215...|\n",
      "|  153.0| 153.03| 152.95|  153.0|27809.0|          0.08000183|                 0.0|\n",
      "+-------+-------+-------+-------+-------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Calculate daily return - how much did the price change in percentage from opening to closing\n",
    "df = df.withColumn(\"daily_return\", (df.close - df.open) / df.open)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e648823b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+-------+--------------------+--------------------+\n",
      "|   open|   high|    low|  close| volume|intra_day_volatility|        daily_return|\n",
      "+-------+-------+-------+-------+-------+--------------------+--------------------+\n",
      "|153.083| 153.09|153.005|153.075|23015.0|         0.084991455|-5.22305276070733...|\n",
      "|  153.1|153.105| 153.01|153.085|33288.0|          0.09500122|-9.79711890951588...|\n",
      "| 152.85| 153.15| 152.81| 153.11|25972.0|          0.33999634|0.001700978059888...|\n",
      "| 152.98| 152.98|  152.8| 152.85|21931.0|          0.17999268|-8.49716483552215...|\n",
      "|  153.0| 153.03| 152.95|  153.0|27809.0|          0.08000183|                 0.0|\n",
      "+-------+-------+-------+-------+-------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Calculate intra-day volatility - difference between the highest and lowest prices of the day\n",
    "df = df.withColumn(\"intra_day_volatility\", df.high - df.low)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "546f9905-0a3b-4d6e-b81d-5cb4013d0691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['open', 'high', 'low', 'close', 'volume']\n",
      "130.66666666666666\n"
     ]
    }
   ],
   "source": [
    "big_list = [[150.61, 151.61, 150.51, 150.51, 130.0],[150.61, 150.61, 150.51, 150.51, 132.0], [150.61, 150.61, 150.51, 150.51, 130.0]]\n",
    "\n",
    "agg_df = spark.createDataFrame(agg_data, schema = schema)\n",
    "print(agg_df.columns)\n",
    "for dlist in big_list:\n",
    "    # agg_data.append(dlist)\n",
    "    temp_df = spark.createDataFrame([dlist], schema = schema)\n",
    "    agg_df = agg_df.union(temp_df)\n",
    "    # rows = agg_df.collect()\n",
    "    # for row in rows:\n",
    "    #     print(row)\n",
    "    mean_value = agg_df.agg(mean(\"volume\")).collect()[0][0]\n",
    "\n",
    "\n",
    "\n",
    "print(mean_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "479eab9c-34c7-4d69-ba46-9cf523de2a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+------+------+\n",
      "|  open|  high|   low| close|volume|\n",
      "+------+------+------+------+------+\n",
      "|150.61|151.61|150.51|150.51| 130.0|\n",
      "|150.61|150.61|150.51|150.51| 132.0|\n",
      "|150.61|150.61|150.51|150.51| 130.0|\n",
      "+------+------+------+------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/02 02:31:34 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "agg_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
